[[wrapper]]
[[__asciidoctor-preview-0__]]
OpenShift 4.2 on OpenStack 13 Installation Lab
----------------------------------------------

[[preamble]]
[[__asciidoctor-preview-1__]]
In this lab, you will install OpenShift 4.2 on Red Hat OpenStack 13

[[__asciidoctor-preview-2__]]
Order your lab environment through https://labs.opentlc.com by selecting *Services* → *Catalogs* → *All Services* → *OPENTLC Cloud Infrastructure Labs* → *rhte2019-ocp-osp*.

[[toc]]
[[toctitle]]
Table of Contents

* link:#_overview[Overview]
* link:#_architecture[Architecture]
** link:#_servers[Servers]
** link:#_networks[Networks]
* link:#_overcloud_preparation[Overcloud preparation]
** link:#_configure_network_and_subnet[Configure network and subnet]
** link:#_create_flavor_code_m1_large_code_and_image_code_rhcos_code[Create flavor `m1.large` and image `rhcos`]
** link:#_create_project_for_openshift_user_and_assign_permissions[Create project for openshift, user and assign permissions]
* link:#_openshift_4_2_installer_preparation[OpenShift 4.2 installer preparation]
** link:#_download_and_configure_go[Download and configure Go]
* link:#_openshift_4_2_installation[OpenShift 4.2 installation]
** link:#_review_installation_process[Review installation process]
* link:#_test_basic_functionality[Test basic functionality]

[[_overview]]
Overview
~~~~~~~~

[[__asciidoctor-preview-3__]]
*OpenShift Container Platform* is a platform for developing and running containerized applications. It is designed to allow applications and the data centers that support them to expand from just a few machines and applications to thousands of machines that serve millions of clients.

[[__asciidoctor-preview-4__]]
In *OpenShift Container Platform 4.1*, you must use *RHCOS* for all control plane machines, but you can use *Red Hat Enterprise Linux* (*RHEL*) as the operating system for compute, or worker, machines. If you choose to use *RHEL* workers, you must perform more system maintenance than if you use RHCOS for all of the cluster machines.

[[__asciidoctor-preview-5__]]
With *OpenShift Container Platform 4.1*, if you have an account with the right permissions, you can deploy a production cluster in supported clouds by running a single command and providing a few values. You can also customize your cloud installation or install your cluster in your data center if you use a supported platform.

[[__asciidoctor-preview-6__]]
For clusters that use *RHCOS* for all machines, updating, or upgrading, *OpenShift Container Platform* is a simple, highly-automated process. Because *OpenShift Container Platform* completely controls the systems and services that run on each machine, including the operating system itself, from a central control plane, upgrades are designed to become automatic events.

[[_architecture]]
Architecture
~~~~~~~~~~~~

[[_servers]]
Servers
^^^^^^^

[cols=",",options="header",]
|=============================================
|Server Name |Description
|workstation |Jump host for the Lab
|undercloud |Red Hat Openstack Director
|ctrl01 |Controller
|compute01 |Compute node (12 CPUs + 32 GB RAM)
|compute02 |Compute node (12 CPUs + 32 GB RAM)
|=============================================

[[_networks]]
Networks
^^^^^^^^

[width="100%",cols="25%,75%",options="header",]
|=============================================================
|Network |Description
|192.168.122.0/24 |LAB Admin Network
|172.16.0.0/24 |OSP Control Plane + Floating IP (flat network)
|10.128.0.0/14 |OCP Cluster Network
|10.10.0.0/16 |OCP Machine network
|172.30.0.0/16 |OCP Service Network
|=============================================================

[[_overcloud_preparation]]
Overcloud preparation
~~~~~~~~~~~~~~~~~~~~~

[[__asciidoctor-preview-9__]]
In this section overcloud will be configured:

[[__asciidoctor-preview-10__]]
* Create a `public` network and subnet for floating ips.
* Create required flavor
* Create a image named `rhcos`
* Create a project named `openshift`
* Create an user named `openshift_admin` and assign role.
* Configure Swift permissions

[[_configure_network_and_subnet]]
Configure network and subnet
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

[[__asciidoctor-preview-11__]]
1.  Connect to `undercloud` (password is `r3dh4t1!` for user `stack`)
+
[[__asciidoctor-preview-12__]]
[Workstation]

[source,nowrap]
----
[student@workstation ~]$ ssh stack@undercloud
----
2.  Create network `public` as external network
+
[[__asciidoctor-preview-13__]]
[source,nowrap]
----
[stack@undercloud ~]$ . overcloudrc
(overcloud) [stack@undercloud ~]$ openstack network create --provider-network-type flat --provider-physical-network datacentre public --external --share
----
+
[[__asciidoctor-preview-14__]]
Sample Output

[source,nowrap]
----
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2019-06-28T08:13:26Z                 |
| description               |                                      |
| dns_domain                | None                                 |
| id                        | 725b7803-45f2-4acd-992d-fa8a1fee28ec |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1500                                 |
| name                      | public                               |
| port_security_enabled     | True                                 |
| project_id                | 6d6f875dd05240ba8ee773f2187f9c11     |
| provider:network_type     | flat                                 |
| provider:physical_network | datacentre                           |
| provider:segmentation_id  | None                                 |
| qos_policy_id             | None                                 |
| revision_number           | 6                                    |
| router:external           | External                             |
| segments                  | None                                 |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2019-06-28T08:13:26Z                 |
+---------------------------+--------------------------------------+
----
3.  Create subnet `public_subnet` to be used for floating IP
+
[[__asciidoctor-preview-15__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack subnet create --no-dhcp --gateway 172.16.0.1 --subnet-range 172.16.0.0/24 --allocation-pool start=172.16.0.50,end=172.16.0.90 --network public public_subnet
----
+
[[__asciidoctor-preview-16__]]
Sample Output

[source,nowrap]
----
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| allocation_pools  | 172.16.0.50-172.16.0.90              |
| cidr              | 172.16.0.0/24                        |
| created_at        | 2019-06-28T08:14:53Z                 |
| description       |                                      |
| dns_nameservers   |                                      |
| enable_dhcp       | False                                |
| gateway_ip        | 172.16.0.1                           |
| host_routes       |                                      |
| id                | e39a115e-c4c4-45cd-8653-4446cb5e8069 |
| ip_version        | 4                                    |
| ipv6_address_mode | None                                 |
| ipv6_ra_mode      | None                                 |
| name              | public_subnet                        |
| network_id        | 725b7803-45f2-4acd-992d-fa8a1fee28ec |
| project_id        | 6d6f875dd05240ba8ee773f2187f9c11     |
| revision_number   | 0                                    |
| segment_id        | None                                 |
| service_types     |                                      |
| subnetpool_id     | None                                 |
| tags              |                                      |
| updated_at        | 2019-06-28T08:14:53Z                 |
+-------------------+--------------------------------------+
----
4.  Create a floating IP
+
[[__asciidoctor-preview-17__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack floating ip create --floating-ip-address 172.16.0.70 public
----
+
[[__asciidoctor-preview-18__]]
Sample Output

[source,nowrap]
----
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| created_at          | 2019-06-28T08:50:30Z                 |
| description         |                                      |
| fixed_ip_address    | None                                 |
| floating_ip_address | 172.16.0.70                          |
| floating_network_id | 725b7803-45f2-4acd-992d-fa8a1fee28ec |
| id                  | 933731d2-554d-43cd-a174-9e4da8c06e7b |
| name                | 172.16.0.70                          |
| port_id             | None                                 |
| project_id          | d4ca6cd93855497c90abb074aef473b5     |
| qos_policy_id       | None                                 |
| revision_number     | 0                                    |
| router_id           | None                                 |
| status              | DOWN                                 |
| subnet_id           | None                                 |
| updated_at          | 2019-06-28T08:50:30Z                 |
+---------------------+--------------------------------------+
----

[[_create_flavor_code_m1_large_code_and_image_code_rhcos_code]]
Create flavor `m1.large` and image `rhcos`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

[[__asciidoctor-preview-19__]]
1.  Create flavor `m1.xlarge` with 6vcpu and 12GB of RAM
+
[[__asciidoctor-preview-20__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack flavor create --ram 12288 --disk 20 --vcpu 6 --public m1.xlarge
----
+
[[__asciidoctor-preview-21__]]
[source,nowrap]
----
+----------------------------+--------------------------------------+
| Field                      | Value                                |
+----------------------------+--------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                |
| OS-FLV-EXT-DATA:ephemeral  | 0                                    |
| disk                       | 20                                   |
| id                         | 6817178e-102e-4440-8897-b381df52311d |
| name                       | m1.xlarge                            |
| os-flavor-access:is_public | True                                 |
| properties                 |                                      |
| ram                        | 12288                                |
| rxtx_factor                | 1.0                                  |
| swap                       |                                      |
| vcpus                      | 6                                    |
+----------------------------+--------------------------------------+
----
2.  Get URL of latest RHCOS image
+
[[__asciidoctor-preview-22__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ curl --silent https://raw.githubusercontent.com/openshift/installer/master/data/data/rhcos.json | jq --raw-output '.baseURI + .images.openstack.path'
----
+
[[__asciidoctor-preview-23__]]
Sample Output

[source,nowrap]
----
https://releases-art-rhcos.svc.ci.openshift.org/art/storage/releases/rhcos-4.2/420.8.20190624.0/rhcos-420.8.20190624.0-openstack.qcow2
----
3.  Download image
+
[[__asciidoctor-preview-24__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ curl --compressed --insecure -L  -O $(curl --silent https://raw.githubusercontent.com/openshift/installer/master/data/data/rhcos.json | jq --raw-output '.baseURI + .images.openstack.path')
----
+
[[__asciidoctor-preview-25__]]
Sample Output

[source,nowrap]
----
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   161  100   161    0     0    344      0 --:--:-- --:--:-- --:--:--   344
100  684M  100  684M    0     0  26.2M      0  0:00:26  0:00:26 --:--:-- 24.7M
----
4.  Create image on Glance
+
[[__asciidoctor-preview-26__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack image create rhcos --container-format bare --disk-format qcow2 --public --file rhcos-*.qcow2
----
+
[[__asciidoctor-preview-27__]]
Sample Output

[source,nowrap]
----
+------------------+------------------------------------------------------------------------------+
| Field            | Value                                                                        |
+------------------+------------------------------------------------------------------------------+
| checksum         | 0a335ea831d6f22dc25236fad2cbb898                                             |
| container_format | bare                                                                         |
| created_at       | 2019-06-28T08:21:33Z                                                         |
| disk_format      | qcow2                                                                        |
| file             | /v2/images/1814a153-ebf5-4ecc-9bc0-871cb965a9a7/file                         |
| id               | 1814a153-ebf5-4ecc-9bc0-871cb965a9a7                                         |
| min_disk         | 0                                                                            |
| min_ram          | 0                                                                            |
| name             | rhcos                                                                        |
| owner            | 6d6f875dd05240ba8ee773f2187f9c11                                             |
| properties       | direct_url='swift+config://ref1/glance/1814a153-ebf5-4ecc-9bc0-871cb965a9a7' |
| protected        | False                                                                        |
| schema           | /v2/schemas/image                                                            |
| size             | 717317607                                                                    |
| status           | active                                                                       |
| tags             |                                                                              |
| updated_at       | 2019-06-28T08:21:48Z                                                         |
| virtual_size     | None                                                                         |
| visibility       | public                                                                       |
+------------------+------------------------------------------------------------------------------+
----

[[_create_project_for_openshift_user_and_assign_permissions]]
Create project for openshift, user and assign permissions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

[[__asciidoctor-preview-28__]]
1.  Create project named `openshift`
+
[[__asciidoctor-preview-29__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack project create openshift
----
+
[[__asciidoctor-preview-30__]]
Sample Output

[source,nowrap]
----
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description |                                  |
| domain_id   | default                          |
| enabled     | True                             |
| id          | d4ca6cd93855497c90abb074aef473b5 |
| is_domain   | False                            |
| name        | openshift                        |
| parent_id   | default                          |
| tags        | []                               |
+-------------+----------------------------------+
----
2.  Create an user named `openshift_admin`
+
[[__asciidoctor-preview-31__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack user create --password 'r3dh4t1!' openshift_admin
----
+
[[__asciidoctor-preview-32__]]
Sample Output

[source,nowrap]
----
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | fe344863375f49d19bc8cb6023042cbd |
| name                | openshift_admin                  |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
----
3.  Assign admin role to the user in the project
+
[[__asciidoctor-preview-33__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack role add --project openshift --user openshift_admin admin
----
+
[[__asciidoctor-preview-34__]]
[cols=",",]
|====================================================================
|__ |This command doesn’t show any output when is executed correctly.
|====================================================================
4.  Assign swiftoperator role to the user in the project
+
[[__asciidoctor-preview-35__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ openstack role add --user openshift_admin --project openshift swiftoperator
----
5.  Create file `openshiftrc` to test authentication
+
[[__asciidoctor-preview-36__]]
Content

[source,nowrap]
----
# Clear any old environment that may conflict.
for key in $( set | awk '{FS="="}  /^OS_/ {print $1}' ); do unset $key ; done
export OS_NO_CACHE=True
export COMPUTE_API_VERSION=1.1
export OS_USERNAME=openshift_admin
export no_proxy=,172.16.0.102,172.16.0.102
export OS_USER_DOMAIN_NAME=Default
export OS_VOLUME_API_VERSION=3
export OS_CLOUDNAME=openshift
export OS_AUTH_URL=http://172.16.0.102:5000//v3
export NOVA_VERSION=1.1
export OS_IMAGE_API_VERSION=2
export OS_PASSWORD=r3dh4t1!
export OS_PROJECT_DOMAIN_NAME=Default
export OS_IDENTITY_API_VERSION=3
export OS_PROJECT_NAME=openshift
export OS_AUTH_TYPE=password
export PYTHONWARNINGS="ignore:Certificate has no, ignore:A true SSLContext object is not available"

# Add OS_CLOUDNAME to PS1
if [ -z "${CLOUDPROMPT_ENABLED:-}" ]; then
    export PS1=${PS1:-""}
    export PS1=\${OS_CLOUDNAME:+"(\$OS_CLOUDNAME)"}\ $PS1
    export CLOUDPROMPT_ENABLED=1
fi
----
6.  Test authentication
+
[[__asciidoctor-preview-37__]]
[source,nowrap]
----
(overcloud) [stack@undercloud ~]$ . openshiftrc
(openshift) [stack@undercloud ~]$ openstack network list
----
+
[[__asciidoctor-preview-38__]]
Sample Output

[source,nowrap]
----
+--------------------------------------+--------+--------------------------------------+
| ID                                   | Name   | Subnets                              |
+--------------------------------------+--------+--------------------------------------+
| 725b7803-45f2-4acd-992d-fa8a1fee28ec | public | e39a115e-c4c4-45cd-8653-4446cb5e8069 |
+--------------------------------------+--------+--------------------------------------+
----
7.  Set temporary url key for `openshift_admin` account
+
[[__asciidoctor-preview-39__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack object store account set --property Temp-URL-Key=RHTETempKey
----
8.  Ensure the temporary key is set
+
[[__asciidoctor-preview-40__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack object store account show
----
+
[[__asciidoctor-preview-41__]]
Sample Output

[source,nowrap]
----
+------------+---------------------------------------+
| Field      | Value                                 |
+------------+---------------------------------------+
| Account    | AUTH_d4ca6cd93855497c90abb074aef473b5 |
| Bytes      | 0                                     |
| Containers | 0                                     |
| Objects    | 0                                     |
| properties | Temp-Url-Key='RHTETempKey'            |
+------------+---------------------------------------+
----

[[_openshift_4_2_installer_preparation]]
OpenShift 4.2 installer preparation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[__asciidoctor-preview-42__]]
In these sections following tasks are performed:

[[__asciidoctor-preview-43__]]
* Download Go and configure it (Tests were done with Go 1.12.6)
* Download latest version of `openshift-installer` and compile it

[[_download_and_configure_go]]
Download and configure Go
^^^^^^^^^^^^^^^^^^^^^^^^^

[[__asciidoctor-preview-44__]]
1.  Download version 1.12.6 of Go
+
[[__asciidoctor-preview-45__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ curl -O https://dl.google.com/go/go1.12.6.linux-amd64.tar.gz
----
+
[[__asciidoctor-preview-46__]]
Sample Output

[source,nowrap]
----
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  122M  100  122M    0     0  86.9M      0  0:00:01  0:00:01 --:--:-- 87.0M
----
2.  Extract Go
+
[[__asciidoctor-preview-47__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ tar -xzf go1.12.6.linux-amd64.tar.gz
----
3.  Check Go binary
+
[[__asciidoctor-preview-48__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ ~/go/bin/go version
----
+
[[__asciidoctor-preview-49__]]
Expected Output

[source,nowrap]
----
go version go1.12.6 linux/amd64
----
4.  Download latest code of OpenShift installer for version 4.2
+
[[__asciidoctor-preview-50__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ mkdir -p src/github.com/openshift/
(openshift) [stack@undercloud ~]$ cd src/github.com/openshift/
(openshift) [stack@undercloud openshift]$ git clone https://github.com/openshift/installer.git -b release-4.2
----
+
[[__asciidoctor-preview-51__]]
Sample Output

[source,nowrap]
----
Cloning into 'installer'...
remote: Enumerating objects: 81408, done.
remote: Total 81408 (delta 0), reused 0 (delta 0), pack-reused 81408
Receiving objects: 100% (81408/81408), 65.30 MiB | 22.48 MiB/s, done.
Resolving deltas: 100% (49102/49102), done.
Checking out files: 100% (15021/15021), done.
----
5.  Configure environment variables, increase timeout and build installer
+
[[__asciidoctor-preview-52__]]
[source,nowrap]
----
(openshift) [stack@undercloud openshift]$ cd installer/
(openshift) [stack@undercloud installer]$ export PATH=~/go/bin/:$PATH GOROOT=~/go/ GOPATH=/home/stack
(openshift) [stack@undercloud installer]$ sed -i 's/30 \* time.Minute/60 * time.Minute/' ./cmd/openshift-install/create.go
(openshift) [stack@undercloud installer]$ hack/build.sh
----
+
[[__asciidoctor-preview-53__]]
Sample Output

[source,nowrap]
----
<<OMITTED>>
+ go generate ./data
writing assets_vfsdata.go
+ echo ' release'
+ grep -q libvirt
+ go build -ldflags ' -X github.com/openshift/installer/pkg/version.Raw=unreleased-master-1199-g06474dd31b58a4e4299473d41a4ab85cd488ebfc -X github.com/openshift/installer/pkg/version.Commit=06474dd31b58a4e4299473d41a4ab85cd488ebfc -s -w' -tags ' release' -o bin/openshift-install ./cmd/openshift-install
----
6.  Test openshift-installer binary
+
[[__asciidoctor-preview-54__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ bin/openshift-install version
----
+
[[__asciidoctor-preview-55__]]
Sample Output

[source,nowrap]
----
bin/openshift-install unreleased-master-1199-g06474dd31b58a4e4299473d41a4ab85cd488ebfc
built from commit 06474dd31b58a4e4299473d41a4ab85cd488ebfc
release image registry.svc.ci.openshift.org/origin/release:4.2
----

[[_openshift_4_2_installation]]
OpenShift 4.2 installation
~~~~~~~~~~~~~~~~~~~~~~~~~~

[[__asciidoctor-preview-56__]]
In this section preparation for the installation and the installation will be performed.

[[__asciidoctor-preview-57__]]
1.  Get `openshift` project id
+
[[__asciidoctor-preview-58__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ export PROJID=$(openstack project show openshift -c id -f value)
----
2.  Generate `clouds.yaml` file (file used for authentication)
+
[[__asciidoctor-preview-59__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ cat > clouds.yaml <<EOF
clouds:
  openstack:
    auth:
      auth_url: http://172.16.0.102:5000/v3
      project_name: openshift
      username: openshift_admin
      password: r3dh4t1!
      user_domain_name: Default
      project_domain_name: Default
      project_id: $PROJID
EOF
----
3.  Create `install-config.yaml` (used for the openshift-install)
+
[[__asciidoctor-preview-60__]]
[source,nowrap]
----
apiVersion: v1
baseDomain: example.com
compute:
- hyperthreading: Enabled
  name: worker
  platform: {}
  replicas: 1
controlPlane:
  hyperthreading: Enabled
  name: master
  platform: {}
  replicas: 1
metadata:
  creationTimestamp: null
  name: rhte
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
    hostSubnetLength: 9
  clusterNetworks:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
    hostSubnetLength: 9
  machineCIDR: 10.10.0.0/16
  networkType: OpenshiftSDN
  serviceCIDR: 172.30.0.0/16
  serviceNetwork:
  - 172.30.0.0/16
  type: OpenshiftSDN
platform:
  openstack:
    cloud: openstack
    computeFlavor: m1.xlarge
    externalNetwork: public
    lbFloatingIP: 172.16.0.70
    region: regionOne
    trunkSupport: "1"
pullSecret: |
  {"auths": { "quay.io": { "auth": "Y29yZW9zK3RlYzJfaWZidWdsa2VndmF0aXJyemlqZGMybnJ5ZzpWRVM0SVA0TjdSTjNROUUwMFA1Rk9NMjdSQUZNM1lIRjRYSzQ2UlJBTTFZQVdZWTdLOUFIQlM1OVBQVjhEVlla", "email": "" }, "registry.redhat.io": {"auth": "NjM0MDA1NnxyaHRlLW9jcDQyLW9zcDEzOmV5SmhiR2NpT2lKU1V6VXhNaUo5LmV5SnpkV0lpT2lJMU1HTmxOMkV6TURFM1pqYzBPV0k1WVRjMU5qVmpPRGhsWlRkaFl6QTNaaUo5LnFQS2MxY0NjSDZUazRxd1BhTlFHWnlaZF8tclBEYTRxMWlYVktxb0VZQkhfb2JZM1U3elVhY2d3UXJHN3R1a1lWLWVUVjlrMzNQUnF3MHA5dVJPRkp6TzltUjdsMVNuUXl4Zm9QUzV3SFM0ZVNtX2o2Tl9vZWV0QnVQRTJlckJkc3p0MVZFRE5kQjlHRXVUVTktMVpneU5GWEt5OGwydnhYSEpQUDNpaXF1MEktdGVIREFuWVhEUW9idkFYVElWUm94NW1RamIzQnpmWjBVYUdqMUxJdXIxQktOaHIwNFEzUjdoTDhuOEFnRXpFWFVfQU5pd25UNGdHak1VZVpsX241SkdBRDFkQ2MyNHR6NTZrLXU3MHYxVzc0dTZkNURnYlpObDhZSXlMZm1xSF9DSFR2WHZCZmozN2xkSmhnVFJGLWk2SF9CVFZRWXRnZk1qbGgtNzk2VHNRZU80RzdRSGFCbUswUG5BS0IzRnVVckRDV21Ka1c0cl9WXy10QmlWYXR4MU5pS1RzVzZ2Y25tU1VQV1lEcElXQzhRc2lybUFNTkV2bzRVQWE0VkptR1F4RjdqZXN3OEh5cFZGVm0yMkJDNWROSjhkemt1R051aDVfNWoxdXA4VUJEYnp4SGpiMl83djB5dkVRQk9NNkUwUk4zcTBkZWVCczBHYWVscUpZbEQ1UmFMSVItWXVOVy1QbDluamVaTjRteVNmcHU4SlpSUndkSFl2ZndKa0trQVpMVG1iQTdWZkFuLVZxVUI1RkpiS01oVmREUl9Mc1BoUkppVTlwQmttWHFKOVNRT05hQ1NlSlNZWU4yWlcwT3QyZzNlMjc4ZXdJSVF4X1dCdm5VTVRXYWpQWU9pOVBQa18yRjZSRWkxbExRVDYtVHpVS3dUa3dUdWI2RjdJ", "email": "6340056|rhte-ocp42-osp13"}}}
sshKey: |
  ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC+BYXIHrVWl8cPYtLzNAEZtMxS5Rh/gtaRlsXLsk1iBA6WesIWhN/So/Tlp7GdGM39EwjwrGEatI1X6R/3okjVV2Uzg6e+x1h+igc+eXcYDxwZm8/2drXrc+Oru1+FBh0AS9KeJBBtmlB6DWIjyo6ipnJ7AN8hknvLsXVbLn//qOeHK7tF5W+Ls8e04CrgujoMa2RgHaW5GMj7mazjzhwoxpXckF8hHavZ4Rxr1CtWD89orH/6PydIj8qn96S7ZqQAP/Q6IVBTXk/6h7rfmdDruzrvWVZnlSyhEWM6LaMNcsLev+iidH4bujEYc1e+hthp14GeDqEFvotaCvZN4Dml stack@undercloud.example.com
----
4.  Configure `/etc/hosts`
+
[[__asciidoctor-preview-61__]]
For this LAB we will use `/etc/hosts` for our DNS names
+
[[__asciidoctor-preview-62__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ cat <<EOF | sudo tee -a /etc/hosts
172.16.0.70 api.rhte.example.com
172.16.0.70 console-openshift-console.apps.rhte.example.com
172.16.0.70 openshift-authentication-openshift-authentication.apps.rhte.example.com
172.16.0.70 django-psql-example-rhte.apps.rhte.example.com
EOF
----
5.  Run installer (creating first working directory)
+
[[__asciidoctor-preview-63__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ mkdir rhte/
(openshift) [stack@undercloud installer]$ cp install-config.yaml rhte/
----

6. Incase you are starting from after creating a new lab, pls. follow this before starting the installation.
+
[source,nowrap]
----
[stack@undercloud openshift]$ . openshiftrc
(openshift) [stack@undercloud openshift]$ cd /home/stack/src/github.com/openshift/installer
(openshift) [stack@undercloud installer]$ export PATH=~/go/bin/:$PATH GOROOT=~/go/ GOPATH=/home/stack
(openshift) [stack@undercloud installer]$ export PROJID=$(openstack project show openshift -c id -f value)
(openshift) [stack@undercloud installer]$ export KUBECONFIG=/home/stack/src/github.com/openshift/installer/rhte/auth/kubeconfig
(openshift) [stack@undercloud installer]$ bin/openshift-install version
----
+
Now run the cluster installation command in a tmux session. This should take around 50 mints.:
+
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ tmux
(openshift) [stack@undercloud installer]$ time bin/openshift-install --log-level=debug create cluster --dir rhte
----
+
[[__asciidoctor-preview-64__]]
Sample Output

[source,nowrap]
----
DEBUG OpenShift Installer unreleased-master-1199-g06474dd31b58a4e4299473d41a4ab85cd488ebfc
DEBUG Built from commit 06474dd31b58a4e4299473d41a4ab85cd488ebfc
DEBUG Fetching "Terraform Variables"...
DEBUG Loading "Terraform Variables"...
DEBUG   Loading "Cluster ID"...
DEBUG     Loading "Install Config"...
DEBUG       Loading "SSH Key"...
DEBUG       Loading "Base Domain"...
DEBUG         Loading "Platform"...
DEBUG       Loading "Cluster Name"...
DEBUG         Loading "Base Domain"...
DEBUG       Loading "Pull Secret"...
DEBUG       Loading "Platform"...
DEBUG     Using "Install Config" loaded from target directory
DEBUG   Loading "Install Config"...
DEBUG   Loading "Image"...
DEBUG     Loading "Install Config"...
DEBUG   Loading "Bootstrap Ignition Config"...
DEBUG     Loading "Install Config"...
DEBUG     Loading "Kubeconfig Admin Client"...
<<OMITTED>>
----

[[__asciidoctor-preview-65__]]
[cols=",",]
|===============================================================================================================
|__ |Installation will take around 50 minutes. Please go to the next section to review the installation process.
|===============================================================================================================

[[_review_installation_process]]
Review installation process
^^^^^^^^^^^^^^^^^^^^^^^^^^^

[[__asciidoctor-preview-66__]]
Open a new session to `undercloud` server.

[[__asciidoctor-preview-67__]]
.  Review created networks during installation
+
[[__asciidoctor-preview-68__]]
[source,nowrap]
----
[stack@undercloud ~]$ . openshiftrc
(openshift) [stack@undercloud ~]$ openstack network list --project openshift
----
+
[[__asciidoctor-preview-69__]]
Sample Output

[source,nowrap]
----
+--------------------------------------+----------------------+----------------------------------------------------------------------------+
| ID                                   | Name                 | Subnets                                                                    |
+--------------------------------------+----------------------+----------------------------------------------------------------------------+
| c96a5d89-f005-47b6-ae74-fd24aba05303 | rhte-sj5bt-openshift | 99665475-bd60-4359-afc3-f637aee2e210, aea8fbce-66f0-4900-927d-518631bf77a3 |
+--------------------------------------+----------------------+----------------------------------------------------------------------------+
----
.  Review created subnets during installation
+
[[__asciidoctor-preview-70__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack subnet list --project openshift
----
+
[[__asciidoctor-preview-71__]]
Sample Output

[source,nowrap]
----
+--------------------------------------+--------------------+--------------------------------------+----------------+
| ID                                   | Name               | Network                              | Subnet         |
+--------------------------------------+--------------------+--------------------------------------+----------------+
| 99665475-bd60-4359-afc3-f637aee2e210 | rhte-sj5bt-service | c96a5d89-f005-47b6-ae74-fd24aba05303 | 10.10.128.0/17 |
| aea8fbce-66f0-4900-927d-518631bf77a3 | rhte-sj5bt-nodes   | c96a5d89-f005-47b6-ae74-fd24aba05303 | 10.10.0.0/17   |
+--------------------------------------+--------------------+--------------------------------------+----------------+
----
.  Review the servers created
+
[[__asciidoctor-preview-72__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack server list
----
+
[[__asciidoctor-preview-73__]]
[source,nowrap]
----
+--------------------------------------+----------------------+--------+-----------------------------------------------+-------+-----------+
| ID                                   | Name                 | Status | Networks                                      | Image | Flavor    |
+--------------------------------------+----------------------+--------+-----------------------------------------------+-------+-----------+
| 1e6634d6-bd51-41b1-ba75-35f6c0e1133a | rhte-sj5bt-master-0  | ACTIVE | rhte-sj5bt-openshift=10.10.0.7                | rhcos | m1.xlarge |
| 32f04305-4065-4249-a035-046911b6dbb2 | rhte-sj5bt-bootstrap | ACTIVE | rhte-sj5bt-openshift=10.10.0.4                | rhcos | m1.xlarge |
| 1660fbed-5c81-436a-b310-09899db5df86 | rhte-sj5bt-api       | ACTIVE | rhte-sj5bt-openshift=10.10.128.3, 172.16.0.70 | rhcos | m1.xlarge |
+--------------------------------------+----------------------+--------+-----------------------------------------------+-------+-----------+
----
+
[[__asciidoctor-preview-74__]]
[cols=",",]
|==============================================================
|__ |Notice the floating IP created, is assigned to the API VM.
|==============================================================
.  List the security groups created
+
[[__asciidoctor-preview-75__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack security group list --project openshift
----
+
[[__asciidoctor-preview-76__]]
Sample Output

[source,nowrap]
----
+--------------------------------------+-------------------+------------------------+----------------------------------+
| ID                                   | Name              | Description            | Project                          |
+--------------------------------------+-------------------+------------------------+----------------------------------+
| 3ceb1aa4-b7ff-4170-b9af-fa332f476129 | default           | Default security group | d4ca6cd93855497c90abb074aef473b5 |
| 486b6ac2-260f-4bd7-967b-1523eba5f530 | rhte-sj5bt-api    |                        | d4ca6cd93855497c90abb074aef473b5 |
| ae3a41c9-f025-4ff7-ac51-2caa39f60633 | rhte-sj5bt-master |                        | d4ca6cd93855497c90abb074aef473b5 |
| bf110af5-be2f-4efc-a957-c3145aab49e8 | rhte-sj5bt-worker |                        | d4ca6cd93855497c90abb074aef473b5 |
+--------------------------------------+-------------------+------------------------+----------------------------------+
----
+
[[__asciidoctor-preview-77__]]
[cols=",",]
|====================================================================================================
|__ |You can explore the rules created using the command ` openstack security group rule list SGNAME`
|====================================================================================================
.  List the trunk ports
+
[[__asciidoctor-preview-78__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack network trunk  list
----
+
[[__asciidoctor-preview-79__]]
Sample Output

[source,nowrap]
----
+--------------------------------------+---------------------------+--------------------------------------+-------------+
| ID                                   | Name                      | Parent Port                          | Description |
+--------------------------------------+---------------------------+--------------------------------------+-------------+
| 172fbbc8-4bd9-49b4-bea6-2cb1c52f4f1a | rhte-sj5bt-master-trunk-0 | 92c5f017-3c20-4c13-8bfd-4ff4be31cbfb |             |
+--------------------------------------+---------------------------+--------------------------------------+-------------+
----
.  Show the trunk parent port
+
[[__asciidoctor-preview-80__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack port show $(openstack network trunk  list -c "Parent Port" -f value)
----
+
[[__asciidoctor-preview-81__]]
Sample Port

[source,nowrap]
----
+-----------------------+--------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                            |
+-----------------------+--------------------------------------------------------------------------------------------------+
| admin_state_up        | UP                                                                                               |
| allowed_address_pairs |                                                                                                  |
| binding_host_id       | overcloud-compute-0.localdomain                                                                  |
| binding_profile       |                                                                                                  |
| binding_vif_details   | bridge_name='tbr-172fbbc8-4', datapath_type='system', ovs_hybrid_plug='True', port_filter='True' |
| binding_vif_type      | ovs                                                                                              |
| binding_vnic_type     | normal                                                                                           |
| created_at            | 2019-06-28T09:41:29Z                                                                             |
| data_plane_status     | None                                                                                             |
| description           |                                                                                                  |
| device_id             | 1e6634d6-bd51-41b1-ba75-35f6c0e1133a                                                             |
| device_owner          | compute:nova                                                                                     |
| dns_assignment        | None                                                                                             |
| dns_name              | None                                                                                             |
| extra_dhcp_opts       |                                                                                                  |
| fixed_ips             | ip_address='10.10.0.7', subnet_id='aea8fbce-66f0-4900-927d-518631bf77a3'                         |
| id                    | 92c5f017-3c20-4c13-8bfd-4ff4be31cbfb                                                             |
| ip_address            | None                                                                                             |
| mac_address           | fa:16:3e:c6:8e:c5                                                                                |
| name                  | rhte-sj5bt-master-port-0                                                                         |
| network_id            | c96a5d89-f005-47b6-ae74-fd24aba05303                                                             |
| option_name           | None                                                                                             |
| option_value          | None                                                                                             |
| port_security_enabled | True                                                                                             |
| project_id            | d4ca6cd93855497c90abb074aef473b5                                                                 |
| qos_policy_id         | None                                                                                             |
| revision_number       | 11                                                                                               |
| security_group_ids    | ae3a41c9-f025-4ff7-ac51-2caa39f60633                                                             |
| status                | ACTIVE                                                                                           |
| subnet_id             | None                                                                                             |
| tags                  | openshiftClusterID=rhte-sj5bt                                                                    |
| trunk_details         | {u'trunk_id': u'172fbbc8-4bd9-49b4-bea6-2cb1c52f4f1a', u'sub_ports': []}                         |
| updated_at            | 2019-06-28T09:43:16Z                                                                             |
+-----------------------+--------------------------------------------------------------------------------------------------+
----
.  List the Swift Container created
+
[[__asciidoctor-preview-82__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack container list
----
+
[[__asciidoctor-preview-83__]]
Sample Output

[source,nowrap]
----
+------------+
| Name       |
+------------+
| rhte-sj5bt |
+------------+
----
.  Check the objects inside
+
[[__asciidoctor-preview-84__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack object list rhte-sj5bt
----
+
[[__asciidoctor-preview-85__]]
Sample Output

[source,nowrap]
----
+-------------------+
| Name              |
+-------------------+
| bootstrap.ign     |
| load-balancer.ign |
+-------------------+
----
+
[[__asciidoctor-preview-86__]]
[cols=",",]
|====================================================================================
|__ |This files are going to be used by RHCOS during the ignition disks boot process.
|====================================================================================
.  Review the `bootstrap.ign` file
+
[[__asciidoctor-preview-87__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ openstack object save rhte-sj5bt bootstrap.ign --file -| jq "." | less
----
. Create `.ssh/config` to use `172.16.0.70` as jump host
+
[[__asciidoctor-preview-88__]]
Content

[source,nowrap]
----
Host 10.10.*
   ProxyJump core@172.16.0.70
----
+
[[__asciidoctor-preview-89__]]
[cols=",",]
|=========================
|__ |Set permission to 640
|=========================
. Connect to the IP of the bootstrap
+
[[__asciidoctor-preview-90__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ ssh core@10.10.0.4
----
. Review the bootstrap process
+
[[__asciidoctor-preview-91__]]
[source,nowrap]
----
[core@bootstrap ~]$   journalctl -b -f -u bootkube.service
----
+
[[__asciidoctor-preview-92__]]
Sample Output

[source,nowrap]
----
Jun 28 09:47:06 bootstrap bootkube.sh[1380]: Starting etcd certificate signer...
Jun 28 09:47:21 bootstrap bootkube.sh[1380]: ac65eb7db95182d365de5cc416bffb90ae923757c28ae84e156494afd18c9769
Jun 28 09:47:21 bootstrap bootkube.sh[1380]: Waiting for etcd cluster...
----

[[__asciidoctor-preview-93__]]
During the bootstrap the `openshift-install` output will show following messages:

[[__asciidoctor-preview-94__]]
[source,nowrap]
----
INFO Waiting up to 30m0s for the Kubernetes API at https://api.rhte.example.com:6443...
DEBUG Still waiting for the Kubernetes API: Get https://api.rhte.example.com:6443/version?timeout=32s: dial tcp 172.16.0.70:6443: connect: connection refused
<<REPEATING>>
DEBUG Still waiting for the Kubernetes API: Get https://api.rhte.example.com:6443/version?timeout=32s: EOF
<<REPEATING>>
DEBUG Still waiting for the Kubernetes API: the server could not find the requested resource
<<REPEATING>>
DEBUG Still waiting for the Kubernetes API: Get https://api.rhte.example.com:6443/version?timeout=32s: EOF
INFO API v1.14.0+1456010 up
INFO Waiting up to 30m0s for bootstrapping to complete...
----

[[__asciidoctor-preview-95__]]
The bootstrap VM will show the following message:

[[__asciidoctor-preview-96__]]
[source,nowrap]
----
Jun 28 09:52:29 bootstrap bootkube.sh[1380]: https://etcd-0.rhte.example.com:2379 is healthy: successfully committed proposal: took = 3.178716ms
Jun 28 09:52:29 bootstrap bootkube.sh[1380]: etcd cluster up. Killing etcd certificate signer...
Jun 28 09:52:30 bootstrap bootkube.sh[1380]: ac65eb7db95182d365de5cc416bffb90ae923757c28ae84e156494afd18c9769
Jun 28 09:52:30 bootstrap bootkube.sh[1380]: Starting cluster-bootstrap...
Jun 28 09:52:35 bootstrap bootkube.sh[1380]: Starting temporary bootstrap control plane...
<<OMITTED>>
Jun 28 09:53:04 bootstrap bootkube.sh[1380]: Created "cluster-dns-02-config.yml" dnses.v1.config.openshift.io/cluster -n
Jun 28 09:53:04 bootstrap bootkube.sh[1380]: Created "cluster-infrastructure-02-config.yml" infrastructures.v1.config.openshift.io/cluster -n
Jun 28 09:53:04 bootstrap bootkube.sh[1380]: Created "cluster-ingress-02-config.yml" ingresses.v1.config.openshift.io/cluster -n
Jun 28 09:53:04 bootstrap bootkube.sh[1380]: Created "cluster-network-02-config.yml" networks.v1.config.openshift.io/cluster -n
Jun 28 09:53:05 bootstrap bootkube.sh[1380]: Created "cluster-proxy-01-config.yaml" proxies.v1.config.openshift.io/cluster -n
Jun 28 09:53:05 bootstrap bootkube.sh[1380]:         Pod Status:openshift-kube-apiserver/kube-apiserver        DoesNotExist
Jun 28 09:53:05 bootstrap bootkube.sh[1380]:         Pod Status:openshift-kube-scheduler/openshift-kube-scheduler        DoesNotExist
Jun 28 09:53:05 bootstrap bootkube.sh[1380]:         Pod Status:openshift-kube-controller-manager/kube-controller-manager        DoesNotExist
Jun 28 09:53:05 bootstrap bootkube.sh[1380]:         Pod Status:openshift-cluster-version/cluster-version-operator        Pending
Jun 28 09:53:05 bootstrap bootkube.sh[1380]: Created "cvo-overrides.yaml" clusterversions.v1.config.openshift.io/version -n openshift-cluster-version
----

[[__asciidoctor-preview-97__]]
At this moment we have to wait till the bootstrap finishes.

[[__asciidoctor-preview-98__]]
.  During this wait, open a new terminal and connect to master node
+
[[__asciidoctor-preview-99__]]
[source,nowrap]
----
(openshift) [stack@undercloud ~]$ ssh core@10.10.0.7
----
.  List the CRI-O pods inside the master
+
[[__asciidoctor-preview-100__]]
[source,nowrap]
----
[core@master-0 ~]$ sudo -i
[root@master-0 ~]# crictl pods ps
----
+
[[__asciidoctor-preview-101__]]
Sample Output

[source,nowrap]
----
POD ID              CREATED             STATE               NAME                                NAMESPACE                    ATTEMPT
26159d2607b4e       2 minutes ago       Ready               sdn-wtlbt                           openshift-sdn                0
03a5684fd0cdc       2 minutes ago       Ready               ovs-csdhm                           openshift-sdn                0
3ef49440bd6d2       2 minutes ago       Ready               sdn-controller-h9dhm                openshift-sdn                0
192e31372da04       3 minutes ago       Ready               multus-wrrbn                        openshift-multus             0
004739b663132       3 minutes ago       Ready               network-operator-6c7d479687-jljvk   openshift-network-operator   0
55bc77972897a       5 minutes ago       Ready               etcd-member-master-0                openshift-etcd               0
----
.  Review the `tun0` device
+
[[__asciidoctor-preview-102__]]
[source,nowrap]
----
    inet 10.128.0.1/23 brd 10.128.1.255 scope global tun0
----
+
[[__asciidoctor-preview-103__]]
Sample Output

[source,nowrap]
----
6: tun0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether ee:02:36:2c:56:9d brd ff:ff:ff:ff:ff:ff
    inet 10.128.0.1/23 brd 10.128.1.255 scope global tun0
       valid_lft forever preferred_lft forever
    inet6 fe80::ec02:36ff:fe2c:569d/64 scope link
       valid_lft forever preferred_lft forever
----
+
[[__asciidoctor-preview-104__]]
[cols=",",]
|=============================================
|__ |IP Corresponds to the OCP Cluster network
|=============================================
.  Review CNI configuration
+
[[__asciidoctor-preview-105__]]
[source,nowrap]
----
[root@master-0 etc]# cat /etc/kubernetes/cni/net.d/80-openshift-network.conf
----
+
[[__asciidoctor-preview-106__]]
Sample Output

[source,nowrap]
----
{
  "cniVersion": "0.3.1",
  "name": "openshift-sdn",
  "type": "openshift-sdn"
}
----
.  Review Multus configuration
+
[[__asciidoctor-preview-107__]]
[source,nowrap]
----
[root@master-0 etc]# cat /etc/kubernetes/cni/net.d/00-multus.conf
----
+
[[__asciidoctor-preview-108__]]
Sample Output

[source,nowrap]
----
{ "name": "multus-cni-network", "type": "multus", "namespaceIsolation": true, "logLevel": "verbose", "kubeconfig": "/etc/kubernetes/cni/net.d/multus.d/multus.kubeconfig", "delegates": [ { "cniVersion": "0.3.1", "name": "openshift-sdn", "type": "openshift-sdn" } ] }
----
+
[[__asciidoctor-preview-109__]]
At this moment you can run `journalctl -f` to see the installation/configuration progress on the master node.

[[__asciidoctor-preview-110__]]
On our `bootstrap` VM you can observe the progress of the required pods creation

[[__asciidoctor-preview-111__]]
[source,nowrap]
----
Jun 28 10:01:50 bootstrap bootkube.sh[1380]:         Pod Status:openshift-kube-apiserver/kube-apiserver        DoesNotExist
Jun 28 10:01:50 bootstrap bootkube.sh[1380]:         Pod Status:openshift-kube-scheduler/openshift-kube-scheduler        DoesNotExist
Jun 28 10:01:50 bootstrap bootkube.sh[1380]:         Pod Status:openshift-kube-controller-manager/kube-controller-manager        Pending
Jun 28 10:01:50 bootstrap bootkube.sh[1380]:         Pod Status:openshift-cluster-version/cluster-version-operator        Ready
Jun 28 10:01:55 bootstrap bootkube.sh[1380]:         Pod Status:openshift-cluster-version/cluster-version-operator        Ready
----

[[__asciidoctor-preview-112__]]
After some minutes the process will finish.

[[__asciidoctor-preview-113__]]
[source,nowrap]
----
Jun 28 10:04:32 bootstrap bootkube.sh[1380]: Skipped "secret-kube-apiserver-to-kubelet-signer.yaml" secrets.v1./kube-apiserver-to-kubelet-signer -n openshift-kube-apiserver-operator as it already exists
Jun 28 10:04:32 bootstrap bootkube.sh[1380]: Skipped "secret-loadbalancer-serving-signer.yaml" secrets.v1./loadbalancer-serving-signer -n openshift-kube-apiserver-operator as it already exists
Jun 28 10:04:32 bootstrap bootkube.sh[1380]: Skipped "secret-localhost-serving-signer.yaml" secrets.v1./localhost-serving-signer -n openshift-kube-apiserver-operator as it already exists
Jun 28 10:04:33 bootstrap bootkube.sh[1380]: Skipped "secret-service-network-serving-signer.yaml" secrets.v1./service-network-serving-signer -n openshift-kube-apiserver-operator as it already exists
Jun 28 10:04:33 bootstrap bootkube.sh[1380]: bootkube.service complete
----

[[__asciidoctor-preview-114__]]
[cols=",",]
|===================================================================================================================================
|__ |When the bootstrap process this VM will be deleted and your connection will be frozen. Open a new connection when this happens.
|===================================================================================================================================

[[__asciidoctor-preview-115__]]
Output of the `openshift-install` command will show the following output:

[[__asciidoctor-preview-116__]]
[source,nowrap]
----
DEBUG Bootstrap status: complete
INFO Destroying the bootstrap resources...
<<OMITTED>>
DEBUG
DEBUG Destroy complete! Resources: 3 destroyed.
INFO Waiting up to 30m0s for the cluster at https://api.rhte.example.com:6443 to initialize...
DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0-0.okd-2019-06-28-044257: 85% complete
----

[[__asciidoctor-preview-117__]]
+

[[__asciidoctor-preview-118__]]
[cols=",",]
|====================================================================================================================================================================================================================================================================
|__ |Ignore errors similar to "Could not update servicemonitor "openshift-service-catalog-controller-manager-operator/openshift-service-catalog-controller-manager-operator" (373 of 397): the server does not recognize this resource, check extension API servers "
|====================================================================================================================================================================================================================================================================

[[__asciidoctor-preview-119__]]
At this moment we can install the `oc` client and see the installation progress.

[[__asciidoctor-preview-120__]]
.  Exit tmux (using `exit` command) and then Download `oc` client
+
[[__asciidoctor-preview-121__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ curl -O https://mirror.openshift.com/pub/openshift-v4/clients/oc/4.2/linux/oc.tar.gz
----
+
[[__asciidoctor-preview-122__]]
Sample Output

[source,nowrap]
----
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 21.6M  100 21.6M    0     0   9.8M      0  0:00:02  0:00:02 --:--:--  9.8M
----
.  Extract it and copy to `/usr/local/bin`
+
[[__asciidoctor-preview-123__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ tar xfz oc.tar.gz
(openshift) [stack@undercloud installer]$ sudo cp oc /usr/local/bin/
----
.  Configure `KUBECONFIG` environment variable and check status
+
[[__asciidoctor-preview-124__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ export KUBECONFIG=/home/stack/src/github.com/openshift/installer/rhte/auth/kubeconfig
(openshift) [stack@undercloud installer]$ oc get clusterversion
----
+
[[__asciidoctor-preview-125__]]
Sample Output

[source,nowrap]
----
NAME      VERSION                         AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.2.0-0.okd-2019-07-22-195548   True        False         4m44s   Cluster version is 4.2.0-0.okd-2019-07-22-195548
----
.  List the cluster operators and the status
+
[[__asciidoctor-preview-126__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ oc get clusteroperator
----
+
[[__asciidoctor-preview-127__]]
Sample Output

[source,nowrap]
----
NAME                                       VERSION                         AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.2.0-0.okd-2019-07-22-195548   True        False         False      7m40s
cloud-credential                           4.2.0-0.okd-2019-07-22-195548   True        False         False      40m
cluster-autoscaler                         4.2.0-0.okd-2019-07-22-195548   True        False         False      7m3s
console                                    4.2.0-0.okd-2019-07-22-195548   True        False         False      7m18s
dns                                        4.2.0-0.okd-2019-07-22-195548   True        False         False      37m
image-registry                             4.2.0-0.okd-2019-07-22-195548   True        False         False      10m
ingress                                    4.2.0-0.okd-2019-07-22-195548   True        False         False      10m
kube-apiserver                             4.2.0-0.okd-2019-07-22-195548   True        False         False      37m
kube-controller-manager                    4.2.0-0.okd-2019-07-22-195548   True        False         False      36m
kube-scheduler                             4.2.0-0.okd-2019-07-22-195548   True        False         False      36m
machine-api                                4.2.0-0.okd-2019-07-22-195548   True        False         False      40m
machine-config                             4.2.0-0.okd-2019-07-22-195548   True        False         False      37m
marketplace                                4.2.0-0.okd-2019-07-22-195548   True        False         False      32m
monitoring                                 4.2.0-0.okd-2019-07-22-195548   True        False         False      6m17s
network                                    4.2.0-0.okd-2019-07-22-195548   True        False         False      36m
node-tuning                                4.2.0-0.okd-2019-07-22-195548   True        False         False      34m
openshift-apiserver                        4.2.0-0.okd-2019-07-22-195548   True        False         False      6m49s
openshift-controller-manager               4.2.0-0.okd-2019-07-22-195548   True        False         False      37m
openshift-samples                          4.2.0-0.okd-2019-07-22-195548   True        False         False      20m
operator-lifecycle-manager                 4.2.0-0.okd-2019-07-22-195548   True        False         False      36m
operator-lifecycle-manager-catalog         4.2.0-0.okd-2019-07-22-195548   True        False         False      36m
operator-lifecycle-manager-packageserver   4.2.0-0.okd-2019-07-22-195548   True        False         False      34m
service-ca                                 4.2.0-0.okd-2019-07-22-195548   True        False         False      40m
service-catalog-apiserver                  4.2.0-0.okd-2019-07-22-195548   True        False         False      34m
service-catalog-controller-manager         4.2.0-0.okd-2019-07-22-195548   True        False         False      34m
storage                                    4.2.0-0.okd-2019-07-22-195548   True        False         False      32m
support                                    4.2.0-0.okd-2019-07-22-195548   True        False         True       40m
----

[[__asciidoctor-preview-128__]]
After some minutes the installation will finish.

[[__asciidoctor-preview-129__]]
[source,nowrap]
----
DEBUG Cluster is initialized
INFO Waiting up to 10m0s for the openshift-console route to be created...
DEBUG Route found in openshift-console namespace: console
DEBUG Route found in openshift-console namespace: downloads
DEBUG OpenShift console route is created
INFO Install complete!
INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/stack/src/github.com/openshift/installer/rhte/auth/kubeconfig'
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.rhte.example.com
INFO Login to the console with user: kubeadmin, password: vUvx4-VghAH-gSpzS-mp4Ip

real    52m44.740s
user    0m39.039s
sys 0m2.374s
----

[[__asciidoctor-preview-130__]]
A new VM was created during the OpenShift installation

[[__asciidoctor-preview-131__]]
1.  Review new VM
+
[[__asciidoctor-preview-132__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ openstack server list --name worker
----
+
[[__asciidoctor-preview-133__]]
Sample Output

[source,nowrap]
----
+--------------------------------------+-------------------------+--------+---------------------------------+-------+-----------+
| ID                                   | Name                    | Status | Networks                        | Image | Flavor    |
+--------------------------------------+-------------------------+--------+---------------------------------+-------+-----------+
| 6dc784d7-bb87-4485-a0a3-c604aa585ca9 | rhte-sj5bt-worker-2djgx | ACTIVE | rhte-sj5bt-openshift=10.10.0.17 | rhcos | m1.xlarge |
+--------------------------------------+-------------------------+--------+---------------------------------+-------+-----------+
----
2.  Review the OC nodes
+
[[__asciidoctor-preview-134__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ oc get nodes
NAME              STATUS   ROLES    AGE   VERSION
host-10-10-0-17   Ready    worker   21m   v1.14.0+04ae0f405
master-0          Ready    master   55m   v1.14.0+04ae0f405
----

[[4dad]]
[[rook-ceph-deployment-on-openshift-4]]
Rook (Ceph) Deployment on OpenShift 4
-------------------------------------
Git Clone my version of Ceph Rook operator configuration files. You can definitely use upstream config files too, however, don’t forget to follow https://rook.io/docs/rook/v0.9/openshift.html?source=post_page---------------------------[OpenShift pre-requisite]. My version of configuration files are tried and tested, should help you move faster.

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
# git clone https://github.com/ritzshah/ocp4-rook.git
# cd ocp4-rook/ceph
----

[[6b34]]
Step — 3: Deploying Ceph cluster using Rook Operator on OCP 4.0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* [[d82c]]
+
List default Security Context Constraints

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$ oc get scc
NAME               AGE
anyuid             38m
hostaccess         38m
hostmount-anyuid   38m
hostnetwork        38m
node-exporter      35m
nonroot            38m
privileged         38m
restricted         38m
----

* [[803c]]
+
Create the security context constraints needed by the Rook pods

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$ oc create -f scc.yaml
securitycontextconstraints.security.openshift.io/rook-ceph created
$(openshift) [stack@undercloud ceph]$ oc get scc
NAME               AGE
anyuid             39m
hostaccess         39m
hostmount-anyuid   39m
hostnetwork        39m
node-exporter      35m
nonroot            39m
privileged         39m
restricted         39m
rook-ceph          23s
----

* [[fc13]]
+
Deploy the rook operator

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$ oc create -f operator.yaml
namespace/rook-ceph-system created
customresourcedefinition.apiextensions.k8s.io/cephclusters.ceph.rook.io created
customresourcedefinition.apiextensions.k8s.io/cephfilesystems.ceph.rook.io created
customresourcedefinition.apiextensions.k8s.io/cephnfses.ceph.rook.io created
customresourcedefinition.apiextensions.k8s.io/cephobjectstores.ceph.rook.io created
customresourcedefinition.apiextensions.k8s.io/cephobjectstoreusers.ceph.rook.io created
customresourcedefinition.apiextensions.k8s.io/cephblockpools.ceph.rook.io created
customresourcedefinition.apiextensions.k8s.io/volumes.rook.io created
clusterrole.rbac.authorization.k8s.io/rook-ceph-cluster-mgmt created
role.rbac.authorization.k8s.io/rook-ceph-system created
clusterrole.rbac.authorization.k8s.io/rook-ceph-global created
clusterrole.rbac.authorization.k8s.io/rook-ceph-mgr-cluster created
serviceaccount/rook-ceph-system created
rolebinding.rbac.authorization.k8s.io/rook-ceph-system created
clusterrolebinding.rbac.authorization.k8s.io/rook-ceph-global created
deployment.apps/rook-ceph-operator created
----

* [[8d05]]
+
Verify Operator pod creation

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$ watch "oc get pods -n rook-ceph-system"
Every 2.0s: oc get pods -n rook-ceph-system                                                                           Tue Jul 23 01:32:58 2019

NAME                                  READY   STATUS              RESTARTS   AGE
rook-ceph-operator-845c97b455-9drpg   0/1     ContainerCreating   0          25s

:
:
Every 2.0s: oc get pods -n rook-ceph-system                                                                           Tue Jul 23 01:33:18 2019

NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-agent-dt77b                 1/1     Running   0          19s
rook-ceph-operator-845c97b455-9drpg   1/1     Running   0          44s
rook-discover-pm9zj                   1/1     Running   0          19s
^C
----

* [[8e78]]
+
Once the operator is ready, you should have the following pods

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$ oc get pods -n rook-ceph-system
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-agent-dt77b                 1/1     Running   0          83s
rook-ceph-operator-845c97b455-9drpg   1/1     Running   0          108s
rook-discover-pm9zj                   1/1     Running   0          83s
----

* [[6abc]]
+
Create rook cluster, which is nothing but a full fledge Ceph cluster including all the daemons

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$  oc create -f cluster.yaml
namespace/rook-ceph created
serviceaccount/rook-ceph-osd created
serviceaccount/rook-ceph-mgr created
role.rbac.authorization.k8s.io/rook-ceph-osd created
role.rbac.authorization.k8s.io/rook-ceph-mgr-system created
role.rbac.authorization.k8s.io/rook-ceph-mgr created
rolebinding.rbac.authorization.k8s.io/rook-ceph-cluster-mgmt created
rolebinding.rbac.authorization.k8s.io/rook-ceph-osd created
rolebinding.rbac.authorization.k8s.io/rook-ceph-mgr created
rolebinding.rbac.authorization.k8s.io/rook-ceph-mgr-system created
rolebinding.rbac.authorization.k8s.io/rook-ceph-mgr-cluster created
cephcluster.ceph.rook.io/rook-ceph created

(openshift) [stack@undercloud ceph]$ watch "oc get pods -n rook-ceph"
Every 2.0s: oc get pods -n rook-ceph                                                                                  Tue Jul 23 01:37:31 2019

NAME                                          READY   STATUS      RESTARTS   AGE
rook-ceph-mgr-a-5ff84bb559-vs4d5              1/1     Running     0	     67s
rook-ceph-mon-a-7685fdbd5f-kc46z              1/1     Running     0	     109s
rook-ceph-mon-b-75cdc7b747-hzt4c              1/1     Running     0	     98s
rook-ceph-mon-c-67c57cb895-85p9q              1/1     Running     0	     86s
rook-ceph-osd-0-7f5886fcdb-742qx              1/1     Running     0          35s
rook-ceph-osd-prepare-host-10-10-0-12-ff54w   0/2     Completed   0          43s
^C

(openshift) [stack@undercloud ceph]$ oc get pods -n rook-ceph
NAME                                          READY   STATUS      RESTARTS   AGE
rook-ceph-mgr-a-5ff84bb559-vs4d5              1/1     Running     0          112s
rook-ceph-mon-a-7685fdbd5f-kc46z              1/1     Running     0          2m34s
rook-ceph-mon-b-75cdc7b747-hzt4c              1/1     Running     0          2m23s
rook-ceph-mon-c-67c57cb895-85p9q              1/1     Running     0          2m11s
rook-ceph-osd-0-7f5886fcdb-742qx              1/1     Running     0          80s
rook-ceph-osd-prepare-host-10-10-0-12-ff54w   0/2     Completed   0          88s

(openshift) [stack@undercloud ceph]$ oc -n rook-ceph get service
NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
rook-ceph-mgr             ClusterIP   172.30.141.235   <none>        9283/TCP   2m5s
rook-ceph-mgr-dashboard   ClusterIP   172.30.8.179     <none>        8443/TCP   2m5s
rook-ceph-mon-a           ClusterIP   172.30.78.217    <none>        6790/TCP   3m9s
rook-ceph-mon-b           ClusterIP   172.30.3.72      <none>        6790/TCP   2m58s
rook-ceph-mon-c           ClusterIP   172.30.91.144    <none>        6790/TCP   2m46s
----

* [[b6a3]]
+
Create a toolbox container to verify your Ceph cluster

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$ oc create -f toolbox.yaml
pod/rook-ceph-tools created

(openshift) [stack@undercloud ceph]$ oc get pods -n rook-ceph
NAME                                          READY   STATUS              RESTARTS   AGE
rook-ceph-mgr-a-5ff84bb559-vs4d5              1/1     Running             0          2m58s
rook-ceph-mon-a-7685fdbd5f-kc46z              1/1     Running             0          3m40s
rook-ceph-mon-b-75cdc7b747-hzt4c              1/1     Running             0          3m29s
rook-ceph-mon-c-67c57cb895-85p9q              1/1     Running             0          3m17s
rook-ceph-osd-0-7f5886fcdb-742qx              1/1     Running             0          2m26s
rook-ceph-osd-prepare-host-10-10-0-12-ff54w   0/2     Completed           0          2m34s
rook-ceph-tools                               0/1     ContainerCreating   0          8s
----

* [[8412]]
+
Login into the container to access your Ceph cluster

[source,hm,hn,ho,hp,hq,fx,do,jq]
----
(openshift) [stack@undercloud ceph]$  oc  -n rook-ceph exec -it rook-ceph-tools bash
bash: warning: setlocale: LC_CTYPE: cannot change locale (en_US.UTF-8): No such file or directory
bash: warning: setlocale: LC_COLLATE: cannot change locale (en_US.UTF-8): No such file or directory
bash: warning: setlocale: LC_MESSAGES: cannot change locale (en_US.UTF-8): No such file or directory
bash: warning: setlocale: LC_NUMERIC: cannot change locale (en_US.UTF-8): No such file or directory
bash: warning: setlocale: LC_TIME: cannot change locale (en_US.UTF-8): No such file or directory

[root@rook-ceph-tools /]# ceph -s
  cluster:
    id:     dd1e0ee0-cc1b-49d6-a870-bd0ad9a35bd8
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum b,a,c
    mgr: a(active)
    osd: 1 osds: 1 up, 1 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   12 GiB used, 7.4 GiB / 19 GiB avail
    pgs:

    [root@rook-ceph-tools /]# ceph status
  cluster:
    id:     dd1e0ee0-cc1b-49d6-a870-bd0ad9a35bd8
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum b,a,c
    mgr: a(active)
    osd: 1 osds: 1 up, 1 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   12 GiB used, 7.4 GiB / 19 GiB avail
    pgs:

----

Once logged into the *toolbox* use commands below to investigate the Ceph status and configuration.

----
ceph status
ceph osd status
ceph osd tree
ceph df
rados df
----

Make sure to `exit` the *toolbox* before continuing.

=== Create Rook storageclass for creating Ceph RBD volumes

In this section you will download *storageclass.yaml* and then create the OCP *storageclass* `rook-ceph-block` that can be used by applications to dynamically claim persistent volumes (*PVCs*). The Ceph pool `replicapool` is created when the OCP *storageclass* is created.

----
curl -O https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4rook/storageclass.yaml
cat  storageclass.yaml
----

Notice the `provisioner: ceph.rook.io/block` and that `replicated: size=2` which is a good practice when there are only 3 OSDs. This is because if one *OSD* is down OCP volumes can continue to be created and used.

----
oc create -f storageclass.yaml
----

Login to *toolbox* pod to run Ceph commands. Compare results for `ceph df` and `rados df` executed in prior section before the new *storageclass* was created. You will see there is now a Ceph pool called `replicapool`. Also, the command `ceph osd pool ls detail` gives you information on how this pool is configured.

----
oc -n rook-ceph rsh $toolbox

ceph df
rados df
rados -p replicapool ls
ceph osd pool ls detail
----

Make sure to `exit` the *toolbox* before continuing.

== Create new OCP deployment using Ceph RBD volume

In this section the `rook-ceph-block` *storageclass* will be used by an OCP application + database deployment to create persistent storage. The persistent storage will be a Ceph RBD (RADOS Block Device) volume (object) in the pool=`replicapool`.

Because the Rails + PostgreSQL deployment uses the `default` *storageclass* you need to modify the current default, gp2, and then make `rook-ceph-block` the default *storageclass*.

----
oc get sc

NAME              PROVISIONER             AGE
gp2 (default)     kubernetes.io/aws-ebs   2d
rook-ceph-block   ceph.rook.io/block      8m27s
----

Now you want to change which *storageclass* is default.

----
oc edit sc gp2
----

Remove this portion shown below from *storageclass* `gp2`. Make sure to note *EXACTLY* where this annotations is located in the *storageclass* (copying this portion to clipboard would be a good idea). The editing tool is `vi` when using *oc edit*. Make sure to save your changes before exiting `:wq!`.

----
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
----

Add the removed portion to `rook-ceph-block` in same place in the file so it will become the `default` *storageclass*. Make sure to save your changes before exiting `:wq!`.

----
oc edit sc rook-ceph-block
----

After editing *storageclass* `rook-ceph-block` the result should be similar to below and `rook-ceph-block` should now be the `default` *storageclass*.

----
oc get sc rook-ceph-block -o yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: "2019-04-26T22:24:29Z"
  name: rook-ceph-block
...omitted...
----

Validate that `rook-ceph-block` is now the default *storageclass* before starting the OCP application deployment.

----
oc get sc

NAME                        PROVISIONER             AGE
gp2                         kubernetes.io/aws-ebs   2d1h
rook-ceph-block (default)   ceph.rook.io/block      10m32s
----

Now you are ready to start the Rails + PostgreSQL deployment and monitor the deployment.

----
oc new-project my-database-app
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
----

After the deployment is started you can monitor with these commands.

----
oc status
oc get pvc -n my-database-app
watch oc get pods -n my-database-app
----

This step could take 5 or more minutes. Wait until there are 2 pods in `Running` STATUS and 4 pods in `Completed` STATUS as shown below.

----
oc get pods -n my-database-app

NAME                                READY   STATUS      RESTARTS   AGE
postgresql-1-deploy                 0/1     Completed   0          5m48s
postgresql-1-lf7qt                  1/1     Running     0          5m40s
rails-pgsql-persistent-1-build      0/1     Completed   0          5m49s
rails-pgsql-persistent-1-deploy     0/1     Completed   0          3m36s
rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          3m28s
rails-pgsql-persistent-1-pjh6q      1/1     Running     0          3m14s
----

Once the deployment is complete you can now test the application and the persistent storage on Ceph. Your `HOST/PORT` will be different.

----
oc get route -n my-database-app

NAME                     HOST/PORT                                                                         PATH   SERVICES                 PORT    TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com          rails-pgsql-persistent
----

Copy your `HOST/PORT` to a browser window to create articles. You will need to append `/articles` to the end.

*Example link:*  http://rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com /articles

Enter the `username` and `password` below to create articles and comments. The articles and comments are saved in a PostgreSQL database which stores its table spaces on the Ceph RBD volume provisioned using the `rook-ceph-block` *storagclass* during the application deployment.

----
username: openshift
password: secret
----

Lets now take another look at the Ceph `replicapool` created by the `rook-ceph-block` *storageclass*. Log into the *toolbox* pod again.

----
oc -n rook-ceph rsh $toolbox
----

Run the same Ceph commands as before the application deployment and compare to results in prior section. Notice the number of objects in `replicapool` now.

----
ceph df
rados df
rados -p replicapool ls | grep pvc
----

Make sure to `exit` the *toolbox*. Validate the OCP *PersistentVolume* (PV) name is the same name as the volume name in the Ceph `replicapool`.

----
oc get pvc -n my-database-app
----



******************************************************************************
IGNORE BELOW SECTION :
******************************************************************************
[[_test_basic_functionality]]
Test basic functionality
~~~~~~~~~~~~~~~~~~~~~~~~

[[__asciidoctor-preview-135__]]
Now is time to test the basic functionality: create an app.

[[__asciidoctor-preview-136__]]
1.  Create a new project named `rhte`
+
[[__asciidoctor-preview-137__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ oc new-project rhte
----
+
[[__asciidoctor-preview-138__]]
Sample Output

[source,nowrap]
----
Now using project "rhte" on server "https://api.rhte.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app django-psql-example

to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
----
2.  Create a new sample apps
+
[[__asciidoctor-preview-139__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ oc new-app django-psql-example
----
+
[[__asciidoctor-preview-140__]]
Sample Output

[source,nowrap]
----
<<OMITTED>>
--> Creating resources ...
    secret "django-psql-example" created
    service "django-psql-example" created
    route.route.openshift.io "django-psql-example" created
    imagestream.image.openshift.io "django-psql-example" created
    buildconfig.build.openshift.io "django-psql-example" created
    deploymentconfig.apps.openshift.io "django-psql-example" created
    service "postgresql" created
    deploymentconfig.apps.openshift.io "postgresql" created
--> Success
    Access your application via route 'django-psql-example-rhte.apps.rhte.example.com'
    Build scheduled, use 'oc logs -f bc/django-psql-example' to track its progress.
    Run 'oc status' to view your app.
----
3.  Check the status
+
[[__asciidoctor-preview-141__]]
[source,nowrap]
----
(openshift) [stack@undercloud installer]$ watch oc status
----
+
[[__asciidoctor-preview-142__]]
Sample Output

[source,nowrap]
----
In project rhte on server https://api.rhte.example.com:6443

http://django-psql-example-rhte.apps.rhte.example.com (svc/django-psql-example)
  dc/django-psql-example deploys istag/django-psql-example:latest <-
    bc/django-psql-example source builds https://github.com/sclorg/django-ex.git on openshift/python:3.6
      build #1 running for 42 seconds - 0905223: Merge pull request #137 from danmcp/patch-1 (Ben Parees <bparees@users.noreply.github.com>)
    deployment #1 waiting on image or update

svc/postgresql - 172.30.174.52:5432
  dc/postgresql deploys openshift/postgresql:10
    deployment #1 waiting on image or update

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
----
+
[[__asciidoctor-preview-143__]]
Wait till the process finished.
4.  Access to the application
+
[[__asciidoctor-preview-144__]]
[source,nowrap]
----
(overcloud) [stack@undercloud installer]$ curl -s django-psql-example-rhte.apps.rhte.example.com | grep title
----
+
[[__asciidoctor-preview-145__]]
[source,nowrap]
----
  <title>Welcome to OpenShift</title>
----
*****************************************************************************
